"""
    Previous Step:
        - A data cleaning pipeline has already been completed.
        - The main training engine must accept the cleaned dataset in one of the following formats:
            * NumPy arrays
            * Pandas DataFrames
            * Vector or matrix representations

    Current Task:
        Develop an AI training pipeline for Clustering, Classification, and Regression tasks.

    Requirements:
        1. Accept pipeline configuration generated by an LLM (in JSON or compatible format).
        2. Validate and parse the configuration before execution.
        3. Implement the complete training pipeline:
            - Data loading from cleaned data output
            - Additional preprocessing (if required)
            - Model selection and initialization
            - Training procedure
            - Evaluation with relevant metrics
            - Model and metadata persistence
        4. Follow SOLID principles to ensure maintainable, modular, and testable code.
        5. Guarantee reproducibility:
            - Set random seeds
            - Log all configuration parameters
            - Save pipeline version and environment details
        6. Include clear, structured logging for each pipeline stage (info, warning, error).
        7. Deliver within a strict 48-hour timeframe from task assignment.

    Notes:
        - The pipeline should be adaptable to multiple ML frameworks if needed (e.g., scikit-learn, XGBoost, LightGBM).
        - All preprocessing steps must be serializable for reuse during inference.
        - The final output should include:
            * Trained model artifacts
            * Evaluation metrics
            * Execution logs
            * Configuration snapshot
"""


import numpy as np
import pandas as pd
import os
import matplotlib
from typing import Optional, Dict, List, Any
import json


from sklearn.model_selection import train_test_split 

from sklearn.linear_model import (
    LogisticRegression,
    LinearRegression,
    Ridge,
    Lasso
)

from sklearn.tree import (
    DecisionTreeClassifier,
    DecisionTreeRegressor
)

from sklearn.ensemble import (
    RandomForestClassifier,
    RandomForestRegressor,
    GradientBoostingClassifier,
    GradientBoostingRegressor,
    AdaBoostClassifier,
    AdaBoostRegressor
)

from sklearn.svm import (
    SVC,
    SVR
)

from sklearn.naive_bayes import (
    GaussianNB
)

from sklearn.neighbors import (
    KNeighborsClassifier,
    KNeighborsRegressor
)

from sklearn.cluster import (
    KMeans
)


MODELS = {
    "Logistic Regression": LogisticRegression,
    "Linear Regression": LinearRegression,
    "Ridge Regression": Ridge,
    "Lasso Regression": Lasso,
    "Decision Tree Classifier": DecisionTreeClassifier,
    "Decision Tree Regressor": DecisionTreeRegressor,
    "Random Forest Classifier": RandomForestClassifier,
    "Random Forest Regressor": RandomForestRegressor,
    "Gradient Boosting Classifier": GradientBoostingClassifier,
    "Gradient Boosting Regressor": GradientBoostingRegressor,
    "AdaBoost Classifier": AdaBoostClassifier,
    "AdaBoost Regressor": AdaBoostRegressor,
    "Support Vector Classifier": SVC,
    "Support Vector Regressor": SVR,
    "Gaussian Naive Bayes": GaussianNB,
    "KNN Classifier": KNeighborsClassifier,
    "KNN Regressor": KNeighborsRegressor,
    "KMeans Clustering": KMeans,
}

class DataTrainingPipelineConfig:
    """
    Configuration for Data Training Pipeline.

    Attributes:
        pipeline_type (str): Type or name of the training pipeline.
        algorithm (str): Name of the ML/DL algorithm to use.
        hyperparameters (Dict[str, Any]): Hyperparameters for the training algorithm.
        cross_validation_folds (int): Number of folds for cross-validation.
        training_data (Optional[Any]): Placeholder for the training dataset.
        validation_split (float): Fraction of training data to be used for validation.
        random_seed (Optional[int]): Seed for reproducibility.
        trained_model (Optional[Any]): Placeholder for the trained model object.
    """

    def __init__(
        self,
        pipeline_type: str = "",
        algorithm: str = "",
        hyperparameters: Optional[Dict[str, Any]] = None,
        cross_validation_folds: int = 5,
        training_data: Optional[Any] = None,
        validation_split: float = 0.2,
        random_seed: Optional[int] = None,
        trained_model: Optional[Any] = None
    ):
        self.pipeline_type = pipeline_type
        self.algorithm = algorithm
        self.hyperparameters = hyperparameters or {}
        self.cross_validation_folds = cross_validation_folds
        self.training_data = training_data
        self.validation_split = validation_split
        self.random_seed = random_seed
        self.trained_model = trained_model

    @property
    def validation_split(self) -> float:
        return self._validation_split

    @validation_split.setter
    def validation_split(self, split: float):
        if not (0 < split < 1):
            raise ValueError("Validation split must be between 0 and 1.")
        self._validation_split = split
